%%-*-latex-*-

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Why study compiler construction?}

Few professionals design and write compilers.

\bigskip

So why teach how to make compilers?
\begin{itemize}

  \item A good software/telecom engineer understands the high-level
  \textbf{languages} as well as the \textbf{hardware}.

  \textbf{A compiler links these two aspects.}

  \item That is why understanding the compiling techniques is
  understanding the interaction between the programming languages and
  the computers.

  \item Many applications embed small languages for configuration
  purposes or make their control versatile (think of macros, scripts,
  data description etc.)

\end{itemize}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Why study compiler construction? (cont)}

The techniques of compilation are necessary for implementing such
languages.

\bigskip

Data formats are also formal languages (languages to specify data),
like HTML, XML, ASN.1 etc.

\bigskip

The compiling techniques are mandatory for reading, treating and
writing data but also to port (migrate) applications
(re-engineering). This is a common task in companies.

\bigskip

Anyway, compilers are excellent examples of complex software systems
\begin{itemize}

  \item which can be rigorously specified,

  \item which only can be implemented by combining theory and
  practice.

\end{itemize}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Function of a compiler}

The function of a compiler is to \textbf{translate} texts written in a
\textbf{source language} into texts written in a \textbf{target
  language}.

\bigskip

Usually, the source language is a \textbf{programming language}, and
the corresponding texts are \textbf{programs}. The target language
is often an \textbf{assembly language}, i.e. a language closer to the
machine language (it is the language understood by the processor) than
the source language.

\bigskip

Some programming languages are compiled into a \textbf{byte-code
  language} instead of assembly. Byte-code is usually not close to any
assembly language. Byte-code is \textbf{interpreted} by another
program, called \textbf{virtual machine} (\textbf{VM}), instead of
being translated to machine language (which is directly executed by
the machine processor): the VM processes the instructions of the
byte-code.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Compilation chain}

From an engineering point of view, the compiler is one link in a chain
of tools:
\begin{center}
\includegraphics[bb=71 587 405 721,scale=0.98]{compilation_chain}
\end{center}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}[containsverbatim]
\frametitle{Compilation chain (cont)}

Let us consider the example of the \textbf{C language}. A famous free
compiler is GNU GCC.

\bigskip

In reality, GCC includes the complete compilation chain, not just a C
compiler:
\begin{itemize}

  \item to only preprocess the sources: \verb+gcc -E prog.c+
  (standard output)\\ 
  Annotations are introduced by \verb+#+, like \verb+#define x 6+
%   You can also call directly the C preprocessor \verb+cpp+.

  \item to preprocess and compile: \verb+gcc -S prog.c+ (output
  \verb+prog.s+)

  \item to preprocess, compile and assemble: \verb+gcc -c prog.c+
  (output \verb+prog.o+)

  \item to preprocess, compile, assemble and link:
  \verb+gcc -o prog prog.c+ (output \verb+prog+)\\
  Linkage can  be directly called using \verb+ld+.

\end{itemize}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{The analysis-synthesis model of compilation}

In this class we shall detail only the compilation stage itself.

\bigskip

There are two parts to compilation: \textbf{analysis} and
\textbf{synthesis}.
\begin{enumerate}
 
  \item The analysis part breaks up the source program into
  constituent pieces of an \textbf{intermediary representation} of the
  program.

  \item The synthesis part constructs the target program from this
  intermediary representation.

\end{enumerate}
In this class we shall restrict ourselves to the analysis part.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Analysis}

The analysis can itself be divided into three successive stages:
\begin{enumerate}

  \item \textbf{linear analysis,} in which the stream of characters
    making up the source program is read and grouped into
    \textbf{lexemes} that are sequences of characters having a
    collective meaning; sets of lexemes with a common interpretation
    are called \textbf{tokens};

  \item \textbf{hierarchical analysis,} in which tokens are grouped
    hierarchically into nested collections (\textbf{trees}) with a
    collective meaning;

  \item \textbf{semantic analysis,} in which certain checks are
    performed to ensure the components of a program fit together
    meaningfully.

\end{enumerate}
In this class we shall focus on linear and hierarchical analysis.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}[containsverbatim]
\frametitle{Lexical analysis}

\label{lexing_eg}

\begin{columns}

  \column{0.5\textwidth} In a compiler, linear analysis is called
  \textbf{lexical analysis} or \textbf{scanning}.

  \bigskip

  During lexical analysis, the characters in the assignment statement
{\small
\begin{verbatim}
position := initial+rate*60
\end{verbatim}
}
  would be grouped into the following lexemes and tokens (see facing
  table).

  \column{0.5\textwidth} The blanks separating the characters of these
  tokens are normally eliminated.

  \bigskip

  \begin{tabular}{l|>{\tt}l}
    \hline\hline
    \multicolumn{1}{c|}{\textsc{Token}}
    & \multicolumn{1}{c}{\textsc{Lexeme}}\\
    \hline
    identifier & position\\
    assignment symbol & :=\\
    identifier & initial\\
    plus sign & +\\
    identifier & rate\\
    multiplication sign & *\\
    number & 60\\
    \hline
  \end{tabular}
\end{columns}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Syntax analysis}

\label{parse_tree_eg}

Hierarchical analysis is called \textbf{parsing} or \textbf{syntax
analysis}. It involves grouping the tokens of the source program
into grammatical phrases that are used by the compiler to synthesize
output. Usually, the grammatical phrases of the source are represented
by a \textbf{parse tree} such as:
\begin{center}
\includegraphics[bb=71 603 338 721]{parse_tree_eg}
\end{center}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}[containsverbatim]
\frametitle{Syntax analysis (cont)}

In the expression 
\begin{verbatim}
initial + rate * 60
\end{verbatim} 
the phrase
\begin{verbatim}
rate * 60
\end{verbatim} 
is a logical unit because the usual conventions of arithmetic
 expressions tell us that multiplication is performed prior to
 addition.

\bigskip

Thus, because the expression 
\begin{verbatim}
initial + rate
\end{verbatim}
is followed by a \verb+*+, it is \textbf{not} grouped into the same
subtree.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}[containsverbatim]
\frametitle{Syntax analysis (cont)}

The hierarchical structure of a program is usually expressed by
\textbf{recursive rules}. For instance, an expression can be defined
by a set of cases:
\begin{enumerate}

  \item Any \emph{identifier} is an expression.\label{rule_id_is_expr}

  \item Any \emph{number} is an expression.\label{rule_num_is_expr}

  \item If \emph{expression}\(_1\) and \emph{expression}\(_2\) are
  expressions, then so are
   \begin{enumerate}
 
     \item \emph{expression}\(_1\) \verb|+|
       \emph{expression}\(_2\) \label{rule_add_is_expr}

     \item \emph{expression}\(_1\) \verb|*|
       \emph{expression}\(_2\) \label{rule_mult_is_expr}

     \item \verb|(| \emph{expression}\(_1\)
       \verb|)| \label{rule_paren_is_expr} 

   \end{enumerate}

\end{enumerate}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}[containsverbatim]
\frametitle{Syntax analysis (cont)}

Rule~\ref{rule_id_is_expr} and~\ref{rule_num_is_expr} are
non-recursive base rules, while the others define expressions in terms
of operators applied to other expressions.

\bigskip

\texttt{initial} and \texttt{rate} are identifiers.

\bigskip

Therefore, by rule~\ref{rule_id_is_expr}, \texttt{initial} and
\texttt{rate} are expressions. 

\bigskip

\texttt{60} is a number.

\bigskip

Thus, by rule~\ref{rule_num_is_expr}, we infer that \texttt{60} is an
expression.

\bigskip

Then, by rule~\ref{rule_mult_is_expr}, we infer that \verb|rate * 60|
is an expression.

\bigskip

Thus, by rule~\ref{rule_add_is_expr}, we conclude that
\verb|initial + rate * 60| is an expression

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}[containsverbatim]
\frametitle{Syntax analysis (cont)}

Similarly, many programming languages define statements recursively
by rules such as
\begin{enumerate}

  \item If \emph{identifier} is an identifier and
    \emph{expression} is an expression, then 
    \begin{center}
      \emph{identifier} \verb|:=| \emph{expression}
    \end{center}
    is a statement.

  \item If \emph{expression} is an expression and \emph{statement} is
    a statement, then
    \begin{center}
       \texttt{while} \texttt{(}\emph{expression}\texttt{)}
       \texttt{do} \emph{statement}\\
       \texttt{if} \texttt{(}\emph{expression}\texttt{)} \texttt{then}
       \emph{statement}
    \end{center}
    are statements.
\end{enumerate}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Syntax analysis (cont)}

The division between lexical and syntactic analysis is somewhat
arbitrary. 

\bigskip

For instance, we could define the integer numbers by means
of recursive rules:
\begin{enumerate}

  \item a \emph{digit} is a \emph{number} (base rule),

  \item a \emph{number} followed by a \emph{digit} is a
  \emph{number} (recursive rule).

\end{enumerate}
Imagine now that the lexer does \textbf{not} recognise numbers, just
 digits. The parser therefore uses the previous recursive rules to
 group in a parse tree the digits which form a number.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Syntax analysis (cont)}
 
\begin{columns}

  \column{0.5\textwidth} For instance, the parse tree for the number
  \texttt{1234}, following these rules, would be
  \begin{center}
    \includegraphics{num_parse_tree}
  \end{center}

  \column{0.5\textwidth}
  But notice how this tree actually is almost a list. 

  \bigskip

  The structure, i.e. the embedding of trees, is indeed not meaningful
  here.

  \bigskip

  For example, there is no obvious meaning to the separation of
  \texttt{12} (same subtree at the leftmost part) in the number
  \texttt{1234}.
\end{columns}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}[containsverbatim]
\frametitle{Syntax analysis (cont)}

Therefore, pragmatically, the best division between the lexer and the
parser is the one that simplifies the overall task of analysis.

\bigskip

One factor in determining the division is whether a source language
construct is inherently recursive or not: lexical constructs do not
require recursion, while syntactic construct often do.

\bigskip

For example, recursion is not necessary to recognise identifiers,
which are typically strings of letters and digits beginning with a
letter: we can read the input stream until a character that is neither
digit nor letter is found, then these read characters are grouped into
an identifier token.

\bigskip

On the other hand, this kind of linear scan is not powerful enough to
analyse expressions or statements, like matching parentheses in
expressions or \verb|{| and \verb|}| in block statements: a nesting
structure is needed.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Syntax analysis (cont)}

\label{ast_eg}

The parse tree page~\pageref{parse_tree_eg} describes the syntactic
structure of the input. A more common \emph{internal} representation
of this syntactic structure is given by
\begin{center}
\includegraphics[bb=71 649 187 721]{ast_eg}
\end{center}
An \textbf{abstract syntax tree} (or just \textbf{syntax tree}) is a
compressed version of the parse tree, where only the most important
elements are retained for the semantic analysis.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Semantic analysis}

The semantic analysis checks the syntax tree for meaningless
constructs and completes it for the synthesis.

\bigskip

An important part of semantic analysis is devoted to
\textbf{type checking}, i.e. checking properties on how the data in
the program is combined.

\bigskip

For instance, many programming languages require an error to be
issued if an array is indexed with a floating-point number (called
\emph{float}).

\bigskip

Some languages allow such floats and integers to be mixed in
arithmetic expressions. Some do not (because internal representation
of integers and floats is very different, as well as the cost of the
corresponding arithmetic functions).

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}[containsverbatim]
\frametitle{Semantic analysis (cont)}

In our example, page~\pageref{ast_eg}, assume all identifiers were
declared as floats. 

\bigskip

The type-checking compares the type of \texttt{rate}, which is float,
and of \texttt{60}, which is integer. Let us assume our language
allows these two types of operands for the multiplication \verb|*|.

\bigskip

Then the analyser must insert a special node in the syntax tree which
represents a \textbf{type cast} from integer to float for
\texttt{60}. 

\bigskip

At the level of the programming language, a type cast is the identity
function (in mathematics: \(x \mapsto x\)), so the value is not
changed, but the type of the result is different from the type of the
argument.

\bigskip

This way the synthesis will know that the assembly code for such a
conversion has to be output.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Semantic analysis (cont)}

\label{annotated_ast_eg}

Hence the semantic analysis issues no error and produces the following
\textbf{annotated syntax tree} for the synthesis:
\begin{center}
\includegraphics{annotated_ast_eg}
\end{center}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases}

\label{phases}

Conceptually, a compiler operates in \textbf{phases}, each of which
transforms the program from one representation to another. A typical
decomposition of a compiler is as follows:
\begin{center}
\includegraphics[bb=71 618 421 721,scale=0.93]{phases}
\end{center}
The first row makes up the analysis and the second the synthesis.
\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases/Symbol table}

The previous figure did not depict another component which is
connected to all the phases: the \textbf{symbol table manager}. A
symbol table is a two-column table whose first column contains
identifiers collected in the program and the second column contains
any interesting information, called \textbf{attributes}, about their
corresponding identifier. Example of identifier attributes are
\begin{itemize}
  \item the allocated storage,

  \item the type, 

  \item the \textbf{scope} (i.e. where in the program it is valid),

  \item in case of procedures names, the number and type of the
  parameters, the method of passing each argument (e.g., by reference)
  and the result type, if any.

\end{itemize}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}[containsverbatim]
\frametitle{Phases/Symbol table (cont)}

When an identifier in the source program is detected by the lexical
analyser (or simply called \textbf{lexer}), this identifier is entered
into the symbol table.

\bigskip

However, some attributes of an identifier cannot normally be
determined during lexical analysis (or simply called
\textbf{lexing}). For example, in a \Pascal declaration like {\small
\begin{verbatim}
var position, initial, rate: real;
\end{verbatim}
}
the type \texttt{real} is not known when \texttt{position},
\texttt{initial} and \texttt{rate} are recognised by the lexical
analyser.

\bigskip

The remaining phases enter information about the identifiers into the
symbol table and use this information. For example, the semantic
analyser needs to know the type of the identifiers to generate
intermediate code.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases/Error detection and reporting}

Another compiler component that was omitted from picture
page~\pageref{phases} because it is also connected to all the phases
is the \textbf{error handler}.

\bigskip

Indeed, each phase can encounter errors, so each phase must somehow
deal with these errors. Here come some examples.
\begin{itemize}

  \item Lexical analysis finds an error if a series of characters do
  not form a token.

  \item Syntax analysis finds an error if the relative position of a
  group of tokens is not described by the grammar (syntax).

  \item Semantic analysis finds an error if the program contains
  the addition a an integer and an array.

\end{itemize}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases/The analysis phase/Lexing}

Let us consider again the analysis phase and its sub-phases in more
details, following a previous example. Consider the next character string
\begin{center}
\(\longleftarrow\)
\texttt{
  \begin{tabular}{|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|@{\,}c@{\,}|}
    \hline
    p&o&s&i&t&i&o&n&\phantom{=}&:&=&\phantom{=}&i&n&i&t&i&a&l&
    \phantom{=}&+&\phantom{=}&r&a&t&e&\phantom{=}&*&\phantom{=}&6&0\\
    \hline
  \end{tabular}
}
\(\longleftarrow\)
\end{center}
First, as we stated page~\pageref{lexing_eg}, the lexical analysis
recognises the tokens of this character string (which can be stored in
a file). The output of the lexing is a stream of tokens like
\begin{center}
\boxedToken{ID}{position} \boxedToken{SYM}{:=}
\boxedToken{ID}{initial} \boxedToken{OP}{+} \boxedToken{ID}{rate}
\boxedToken{OP}{*} \boxedToken{NUM}{60}
\end{center}
where \tokenName{ID} (\emph{identifier}), \tokenName{SYM}
(\emph{symbol}), \tokenName{OP} (\emph{operator}) and \tokenName{NUM}
(\emph{number}) are the token names and between brackets are the
\textbf{lexemes}.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases/The analysis phase/Lexing (cont)}

The lexer also outputs or updates a symbol table like\footnote{Even if
  the table is named ``symbol table'' it actually contains information
  about identifiers only.}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Identifier & Attributes\\
\hline
\hline
\texttt{position} & \ldots\\ 
\hline
\texttt{initial} & \ldots\\
\hline
\texttt{rate} & \ldots\\
\hline
\end{tabular}
\end{center}
The attributes often include the position of the corresponding
identifier in the original string, like the position of the first
character either counting from the start of the string or through the
line and column numbers.
\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases/The analysis phase/Parsing}

\begin{columns}

  \column{0.5\textwidth} The parser takes this token stream and
  outputs the corresponding syntax tree and/or report errors.

  \bigskip

  Page~\pageref{ast_eg}, we gave a simplified version of this syntax
  tree. A refined version is given in the facing column.

  \bigskip

  Also, if the language requires variable definitions, the syntax
  analyser can complete the symbol table with the type of the
  identifiers.

  \column{0.5\textwidth}
  \begin{center}
    \includegraphics[bb=71 619 218 721]{refined_ast_eg}
  \end{center}

\end{columns}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases/The analysis phase/Parsing (cont)}

The parse tree can be considered as a \textbf{trace} of the syntax
analysis process: it summarises all the recognition work done by the
parser. It depends on the syntax rules (i.e. the grammar) and the
input stream of tokens. 
%We will discuss later how to define a grammar.
\begin{center}
\includegraphics[bb=71 605 377 721]{refined_parse_tree_eg}
\end{center}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases/The analysis phase/Semantic analysis}

The semantic analysis considers the syntax tree and checks certain
properties depending on the language, typically it makes sure that the
valid syntactic constructs also have a certain meaning (with respect
to the rules of the language). 

\bigskip

We saw page~\pageref{annotated_ast_eg} that this phase can annotate or
even add nodes to the syntax tree. It can as well update the symbol
table with the information newly gathered in order to facilitate the
code generation and/or optimisation.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases/The analysis phase/Semantic
  analysis (cont)}

Assuming that our toy language accepts that an integer is mixed with
floats in arithmetic operations, the semantic analysis can insert a
type cast node. A new version of the annotated syntax tree would be:
\begin{center}
\includegraphics[bb=71 629 270 721]{refined_annotated_ast_eg}
\end{center}
Note that the new node is not a token, just a (semantic) tag for the
code generator.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases/The synthesis phase}

The purpose of the synthesis phase is to use all the information
gathered by the analysis phase in order to produce the code in the
target language.

\bigskip

Given the annotated syntax tree and the symbol table, the first
sub-phase consists in producing a program in some artificial,
intermediary, language. 

\bigskip

Such a language should be independent of the target language, while
containing features common to the \emph{family} the target language
belongs to. 

\bigskip

For instance, if the target language is the PowerPC G4 microprocessor,
the intermediary language should be like an assembly of the IBM RISC
family.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}[containsverbatim]
\frametitle{Phases/The synthesis phase (cont)}

If we want \textbf{to port a compiler} from one platform to another,
i.e., make it generate code for a different OS or processor, such
intermediary language comes handy: if the new platform share some
features with the first one, we only have to rewrite the code
generator component of the compiler --- not the whole compiler.

\bigskip

It may be interesting to have the same intermediary language for
different source languages, allowing the sharing of the synthesis.

\bigskip

We can think of an intermediary language as an assembly for an
\textbf{abstract machine} (or processor). For instance, our example
could lead to the code
{\small
\begin{verbatim}
temp1 := inttoreal(60)
temp2 := id_rate * temp1
temp3 := id_initial + temp2
id_position := temp3
\end{verbatim}
}
\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases/The synthesis phase (cont)}

Another point of view is to consider the intermediary code as a tiny
subset of the source language, as it retains some high-level features
from it, like, in our example, variables (instead of explicit storage
information, like memory addresses or register numbers), operator
names etc.

\bigskip

This point of view enables optimisations that otherwise would be
harder to achieve (because too many aspects would depend closely on
many details of the target architecture).

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases/The synthesis phase (cont)}

This kind of assembly is called \textbf{three-address code}. It has
several properties:
\begin{itemize}

  \item each instruction has at most one operator (in addition to the
  assignment);

  \item each instruction can have at most three operands;

  \item some instructions can have less than three operands (e.g. the
  first and last instruction);

  \item the result of an operation must be linked to a variable; 

\end{itemize}
As a consequence, the compiler must order well the code for the
sub-expressions, e.g. the second instruction must come before the
third one because the multiplication has priority on addition.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}[containsverbatim]
\frametitle{Phases/The synthesis phase/Code optimisation}

\label{opt_code}

The code optimisation phase attempts to improve the intermediate code,
so that faster-running target code will result.

\bigskip

The code optimisation produces intermediate code: the output language
is the same as the input language.

\bigskip

For instance, this phase would find out that our little program
would be more efficient this way:
{\small
\begin{verbatim}
temp1 := id_rate * 60.0
id_position := id_initial + temp1
\end{verbatim}
}
This simple optimisation is based on the fact that type casting can be
performed at compile-time instead of run-time, but it would be an
unnecessary concern to integrate it in the code generation phase.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}[containsverbatim]
\frametitle{Phases/The synthesis phase/Code generation}

The code generation is the last phase of a compiler. It consists in
the generation of target code, usually relocatable assembly code, from
the optimised intermediate code.

\bigskip

A crucial aspect is the assignment of variables to registers.

\bigskip

\begin{columns}
  \column{0.5\textwidth} For example, the translation of code
  page~\pageref{opt_code} could be
{\small
\begin{verbatim}
MOVF id_rate, R2
MULF #60.0, R2
MOVF id_initial, R1
ADDF R2, R1
MOVF R1, id_position
\end{verbatim}
}

  \column{0.5\textwidth} The first and second operands specify
  respectively a source and a destination.

  \bigskip

  The \texttt{F} in each instruction tells us that the instruction is
  dealing with floating-point numbers.
\end{columns}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Phases/The synthesis phase/Code generation (cont)}

This code moves the contents of the address \texttt{id\_rate} into
register \texttt{2}, then multiplies it with the float 60.0.

\bigskip

The \texttt{\#} signifies that \texttt{60.0} is a constant. 

\bigskip

The third instruction moves \texttt{id\_initial} into register
\texttt{1} and adds to it the value previously computed in register
\texttt{2}.

\bigskip

Finally, the value in register \texttt{1} is moved to the address of
\texttt{id\_position}.

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Implementation of phases into passes}

An implementation of the analysis is called a \textbf{front-end} and
an implementation of the synthesis \textbf{back-end}.

\bigskip

A \textbf{pass} consists in reading an input file and writing an output
file. 

\bigskip

It is possible to group several phases into one pass in order to
interleave their activity.
\begin{itemize}

  \item On one hand, this can lead to a greater efficiency since
  interactions with the file system are much slower than with internal
  memory. 

  \item On the other hand, this architecture leads to a greater
  complexity of the compiler --- something the software engineer
  always fears.

\end{itemize}

\end{frame}

% ------------------------------------------------------------------------
% 
\begin{frame}
\frametitle{Implementation of phases into passes (cont)}

Sometimes it is difficult to group different phases into one pass.

\bigskip

For example, the interface between the lexer and the parser is often a
single token. There is not a lot of activity to interleave: the
parser requests a token to the lexer which computes it and gives it
back to the parser. In the meantime, the parser had to wait.

\bigskip

Similarly, it is difficult to generate the target code if the
intermediate code is not fully generated first. Indeed, some languages
allow the use of variables without a prior declaration, so we cannot
generate immediately the target code because this requires the
knowledge of the variable type.

\end{frame}
